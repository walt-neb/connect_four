

# filename: h1.hyp
# training inputs for DDQN connect-four game player using NNs 
# which are constructed of CNN layers followed by FC layers
# NOTE: the equal sign is used as a delimeter, and should not exist 
# anywhere in this file except where hyperparameters are set

start_episode = 0
end_episode = 100000
console_status_interval = 500
tensorboard_status_interval = 100
ckpt_interval = 2500
render_game_at = [1, 250001]
enable_reward_shaping = True
env_debug_mode = False


agent1_learning_rate = 0.00025
agent2_learning_rate = 0.00025
a1_epsilon_start = .9
a1_epsilon_end = 0.01
a2_epsilon_start = .9
a2_epsilon_end = 0.01
batch_size = 32
gamma = 0.99
tau = 0.005
max_replay_buffer_size = 100000


# Convolutional layers: (out_channels, kernel_size, stride, padding)
cnn_a1 = [(16, 4, 1, 1)]
cnn_a2 = [(16, 4, 1, 1)] 

# Fully connected layer dimensions for each agent
fc_a1 = [480, 128, 64]
fc_a2 = [480, 128, 64]

NOTE: Calculate the output size for each convolutional layer using the formula:
CNN_Output_Size is equal to ((Input_Size−Kernel_Size+2×Padding)/Stride)+1
Make sure the FC_input_size matches the CNN_output_Size





----------- Training Results ----------------
Started training at: 	2024-05-14  21:45:49
Ended training at: 	2024-05-14  21:46:23
Total training time:  0:00:34.146455
start_episode: 0
end_episode: 100000
Episode count: 700
agent1 end epsilon: 0.872053781060567
agent2 end epsilon: 0.872053781060567
Draws: 2
agent_1_starts / agent_2_starts 1.061764705882353
agent_1_reward / agent_2_reward 0.9767716535433071
Ave steps per game: 20.99
total_loss1 / num_steps1: 0.03616830679987158
total_loss2 / num_steps2: 0.04697647504508495
Input Parameters:
		start_episode :	0
		end_episode :	100000
		console_status_interval :	500
		tensorboard_status_interval :	100
		ckpt_interval :	2500
		render_game_at :	[1, 250001]
		enable_reward_shaping :	True
		env_debug_mode :	False
		agent1_learning_rate :	0.00025
		agent2_learning_rate :	0.00025
		a1_epsilon_start :	0.9
		a1_epsilon_end :	0.01
		a2_epsilon_start :	0.9
		a2_epsilon_end :	0.01
		batch_size :	32
		gamma :	0.99
		tau :	0.005
		max_replay_buffer_size :	100000
		cnn_a1 :	[(16, 4, 1, 1)]
		cnn_a2 :	[(16, 4, 1, 1)]
		fc_a1 :	[480, 128, 64]
		fc_a2 :	[480, 128, 64]
models saved for both agents:
./wts/m1_h3_cnn
./wts/m2_h3_cnn
replay buffer saved to
./wts/replay_buffer_h3_cnn.pkl




----------- Training Results ----------------
Started training at: 	2024-05-14  21:47:12
Ended training at: 	2024-05-14  23:16:03
Total training time:  1:28:51.051353
start_episode: 700
end_episode: 100000
Episode count: 99999
agent1 end epsilon: 0.010320000006531953
agent2 end epsilon: 0.010320000006531953
Draws: 37
agent_1_starts / agent_2_starts 1.0004431999033019
agent_1_reward / agent_2_reward 3.737996657465017
Ave steps per game: 23.58
total_loss1 / num_steps1: 0.002741421508188877
total_loss2 / num_steps2: 0.001349189312249008
Input Parameters:
		start_episode :	700
		end_episode :	100000
		console_status_interval :	500
		tensorboard_status_interval :	100
		ckpt_interval :	2500
		render_game_at :	[1, 250001]
		enable_reward_shaping :	True
		env_debug_mode :	False
		agent1_learning_rate :	0.00025
		agent2_learning_rate :	0.00025
		a1_epsilon_start :	0.9
		a1_epsilon_end :	0.01
		a2_epsilon_start :	0.9
		a2_epsilon_end :	0.01
		batch_size :	32
		gamma :	0.99
		tau :	0.005
		max_replay_buffer_size :	100000
		cnn_a1 :	[(16, 4, 1, 1)]
		cnn_a2 :	[(16, 4, 1, 1)]
		fc_a1 :	[480, 128, 64]
		fc_a2 :	[480, 128, 64]
models saved for both agents:
./wts/m1_h3_cnn
./wts/m2_h3_cnn
replay buffer saved to
./wts/replay_buffer_h3_cnn.pkl




----------- Training Results ----------------
Started training at: 	2024-05-15  05:54:49
Ended training at: 	2024-05-15  07:38:46
Total training time:  1:43:56.691938
start_episode: 0
end_episode: 100000
Episode count: 99999
agent1 end epsilon: 0.01
agent2 end epsilon: 0.01
Draws: 61
agent_1_starts / agent_2_starts 0.9934217083624041
agent_1_reward / agent_2_reward 2.320220964159964
Ave steps per game: 30.97
total_loss1 / num_steps1: 0.0038513977779075503
total_loss2 / num_steps2: 0.001171805228188853
Input Parameters:
		start_episode :	0
		end_episode :	100000
		console_status_interval :	500
		tensorboard_status_interval :	100
		ckpt_interval :	2500
		render_game_at :	[1, 250001]
		enable_reward_shaping :	True
		env_debug_mode :	False
		agent1_learning_rate :	0.00025
		agent2_learning_rate :	0.00025
		a1_epsilon_start :	0.9
		a1_epsilon_end :	0.01
		a2_epsilon_start :	0.9
		a2_epsilon_end :	0.01
		batch_size :	32
		gamma :	0.99
		tau :	0.005
		max_replay_buffer_size :	100000
		cnn_a1 :	[(16, 4, 1, 1)]
		cnn_a2 :	[(16, 4, 1, 1)]
		fc_a1 :	[480, 128, 64]
		fc_a2 :	[480, 128, 64]
models saved:
./wts/m1_h1
replay buffer saved to
./wts/replay_buffer_h1.pkl

