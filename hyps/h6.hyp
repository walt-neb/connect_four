

# filename: h5.hyp
# training inputs for DDQN connect-four game player using NNs 
# which are constructed of CNN layers followed by FC layers
# NOTE: the equal sign is used as a delimeter, and should not exist 
# anywhere in this file except where hyperparameters are set

start_episode = 0
end_episode = 200
console_status_interval = 500
tensorboard_status_interval = 100
ckpt_interval = 25000
render_game_at = [1, 2102, 200001]
enable_reward_shaping = False
env_debug_mode = False


agent1_learning_rate = 0.00025
agent2_learning_rate = 0.00025
a1_epsilon_start = .19
a1_epsilon_end = 0.01
a2_epsilon_start = .19
a2_epsilon_end = 0.01
batch_size = 32
gamma = 0.99
tau = 0.005
sequence_length = 42
max_replay_buffer_size = 320000


# Convolutional layers: (out_channels, kernel_size, stride, padding)
cnn_a1 = [(16, 3, 1, 1)]
cnn_a2 = [(16, 3, 1, 1)]

# Fully connected layer dimensions for each agent
fc_a1 = [672]  
fc_a2 = [672]






----------- Training Results ----------------
Started training at: 	2024-05-21  10:27:35
Ended training at: 	2024-05-21  10:27:35
Total training time:  0:00:00.763943
start_episode: 0
end_episode: 2
Episode count: 1
agent1 end epsilon: 0.01
agent2 end epsilon: 0.01
Draws: 0
agent_1_starts / agent_2_starts 0.0
agent_1_reward / agent_2_reward 0.0
Ave steps per game: 9.82
total_loss1 / num_steps1: 0.0
total_loss2 / num_steps2: 0.0
Input Parameters:
		start_episode :	0
		end_episode :	2
		console_status_interval :	500
		tensorboard_status_interval :	100
		ckpt_interval :	25000
		render_game_at :	[1, 2102, 200001]
		enable_reward_shaping :	False
		env_debug_mode :	False
		agent1_learning_rate :	0.00025
		agent2_learning_rate :	0.00025
		a1_epsilon_start :	0.19
		a1_epsilon_end :	0.01
		a2_epsilon_start :	0.19
		a2_epsilon_end :	0.01
		batch_size :	32
		gamma :	0.99
		tau :	0.005
		sequence_length :	4
		max_replay_buffer_size :	100000
		cnn_a1 :	[(16, 3, 1, 1)]
		cnn_a2 :	[(16, 3, 1, 1)]
		fc_a1 :	[2688, 1024, 512, 256, 128]
		fc_a2 :	[2688, 1024, 512, 256, 128]



----------- Training Results ----------------
Started training at: 	2024-05-21  10:28:40
Ended training at: 	2024-05-21  10:28:41
Total training time:  0:00:01.314831
start_episode: 0
end_episode: 2
Episode count: 1
agent1 end epsilon: 0.01
agent2 end epsilon: 0.01
Draws: 0
Ave steps per game: 10.53
total_loss1 / num_steps1: 0.0
total_loss2 / num_steps2: 0.0
Input Parameters:
		start_episode :	0
		end_episode :	2
		console_status_interval :	500
		tensorboard_status_interval :	100
		ckpt_interval :	25000
		render_game_at :	[1, 2102, 200001]
		enable_reward_shaping :	False
		env_debug_mode :	False
		agent1_learning_rate :	0.00025
		agent2_learning_rate :	0.00025
		a1_epsilon_start :	0.19
		a1_epsilon_end :	0.01
		a2_epsilon_start :	0.19
		a2_epsilon_end :	0.01
		batch_size :	32
		gamma :	0.99
		tau :	0.005
		sequence_length :	4
		max_replay_buffer_size :	100000
		cnn_a1 :	[(16, 3, 1, 1)]
		cnn_a2 :	[(16, 3, 1, 1)]
		fc_a1 :	[2688, 1024, 512, 256, 128]
		fc_a2 :	[2688, 1024, 512, 256, 128]



----------- Training Results ----------------
Started training at: 	2024-05-21  10:29:40
Ended training at: 	2024-05-21  10:29:42
Total training time:  0:00:01.795637
start_episode: 0
end_episode: 50
Episode count: 49
agent1 end epsilon: 0.01
agent2 end epsilon: 0.01
Draws: 0
agent_1_starts / agent_2_starts 1.0833333333333333
agent_1_reward / agent_2_reward 1.380952380952381
Ave steps per game: 12.67
total_loss1 / num_steps1: 0.0
total_loss2 / num_steps2: 0.0
Input Parameters:
		start_episode :	0
		end_episode :	50
		console_status_interval :	500
		tensorboard_status_interval :	100
		ckpt_interval :	25000
		render_game_at :	[1, 2102, 200001]
		enable_reward_shaping :	False
		env_debug_mode :	False
		agent1_learning_rate :	0.00025
		agent2_learning_rate :	0.00025
		a1_epsilon_start :	0.19
		a1_epsilon_end :	0.01
		a2_epsilon_start :	0.19
		a2_epsilon_end :	0.01
		batch_size :	32
		gamma :	0.99
		tau :	0.005
		sequence_length :	4
		max_replay_buffer_size :	100000
		cnn_a1 :	[(16, 3, 1, 1)]
		cnn_a2 :	[(16, 3, 1, 1)]
		fc_a1 :	[2688, 1024, 512, 256, 128]
		fc_a2 :	[2688, 1024, 512, 256, 128]



----------- Training Results ----------------
Started training at: 	2024-05-21  12:07:08
Ended training at: 	2024-05-21  12:07:10
Total training time:  0:00:02.159195
start_episode: 0
end_episode: 100
Episode count: 99
agent1 end epsilon: 0.01
agent2 end epsilon: 0.01
Draws: 0
agent_1_starts / agent_2_starts 0.8867924528301887
agent_1_reward / agent_2_reward 1.0
Ave steps per game: 7.22
total_loss1 / num_steps1: 0.0
total_loss2 / num_steps2: 0.0
Input Parameters:
		start_episode :	0
		end_episode :	100
		console_status_interval :	500
		tensorboard_status_interval :	100
		ckpt_interval :	25000
		render_game_at :	[1, 2102, 200001]
		enable_reward_shaping :	False
		env_debug_mode :	False
		agent1_learning_rate :	0.00025
		agent2_learning_rate :	0.00025
		a1_epsilon_start :	0.19
		a1_epsilon_end :	0.01
		a2_epsilon_start :	0.19
		a2_epsilon_end :	0.01
		batch_size :	32
		gamma :	0.99
		tau :	0.005
		sequence_length :	4
		max_replay_buffer_size :	100000
		cnn_a1 :	[(16, 3, 1, 1)]
		cnn_a2 :	[(16, 3, 1, 1)]
		fc_a1 :	[2688, 1024, 512, 256, 128]
		fc_a2 :	[2688, 1024, 512, 256, 128]



----------- Training Results ----------------
Started training at: 	2024-05-23  19:50:59
Ended training at: 	2024-05-23  19:51:07
Total training time:  0:00:08.335994
start_episode: 0
end_episode: 100
Episode count: 99
agent1 end epsilon: 0.01
agent2 end epsilon: 0.01
Draws: 0
agent_1_starts / agent_2_starts 0.8867924528301887
agent_1_reward / agent_2_reward 1.0
Ave steps per game: 7.31
total_loss1 / num_steps1: 0.0
total_loss2 / num_steps2: 0.0
Input Parameters:
		start_episode :	0
		end_episode :	100
		console_status_interval :	500
		tensorboard_status_interval :	100
		ckpt_interval :	25000
		render_game_at :	[1, 2102, 200001]
		enable_reward_shaping :	False
		env_debug_mode :	False
		agent1_learning_rate :	0.00025
		agent2_learning_rate :	0.00025
		a1_epsilon_start :	0.19
		a1_epsilon_end :	0.01
		a2_epsilon_start :	0.19
		a2_epsilon_end :	0.01
		batch_size :	32
		gamma :	0.99
		tau :	0.005
		sequence_length :	4
		max_replay_buffer_size :	100000
		cnn_a1 :	[(16, 3, 1, 1)]
		cnn_a2 :	[(16, 3, 1, 1)]
		fc_a1 :	[2688, 1024, 512, 256, 128]
		fc_a2 :	[2688, 1024, 512, 256, 128]



----------- Training Results ----------------
Started training at: 	2024-05-23  19:51:35
Ended training at: 	2024-05-23  19:51:46
Total training time:  0:00:10.918070
start_episode: 0
end_episode: 100
Episode count: 99
agent1 end epsilon: 0.01
agent2 end epsilon: 0.01
Draws: 0
agent_1_starts / agent_2_starts 0.7857142857142857
agent_1_reward / agent_2_reward 0.05263157894736842
Ave steps per game: 11.74
total_loss1 / num_steps1: 0.0
total_loss2 / num_steps2: 0.0
Input Parameters:
		start_episode :	0
		end_episode :	100
		console_status_interval :	500
		tensorboard_status_interval :	100
		ckpt_interval :	25000
		render_game_at :	[1, 2102, 200001]
		enable_reward_shaping :	False
		env_debug_mode :	False
		agent1_learning_rate :	0.00025
		agent2_learning_rate :	0.00025
		a1_epsilon_start :	0.19
		a1_epsilon_end :	0.01
		a2_epsilon_start :	0.19
		a2_epsilon_end :	0.01
		batch_size :	32
		gamma :	0.99
		tau :	0.005
		sequence_length :	4
		max_replay_buffer_size :	100000
		cnn_a1 :	[(16, 3, 1, 1)]
		cnn_a2 :	[(16, 3, 1, 1)]
		fc_a1 :	[2688, 1024, 512, 256, 128]
		fc_a2 :	[2688, 1024, 512, 256, 128]



----------- Training Results ----------------
Started training at: 	2024-05-23  20:02:36
Ended training at: 	2024-05-23  20:03:25
Total training time:  0:00:48.860153
start_episode: 0
end_episode: 200
Episode count: 199
agent1 end epsilon: 0.01
agent2 end epsilon: 0.01
Draws: 0
agent_1_starts / agent_2_starts 1.0
agent_1_reward / agent_2_reward 0.9801980198019802
Ave steps per game: 7.09
total_loss1 / num_steps1: 0.0
total_loss2 / num_steps2: 0.0
Input Parameters:
		start_episode :	0
		end_episode :	200
		console_status_interval :	500
		tensorboard_status_interval :	100
		ckpt_interval :	25000
		render_game_at :	[1, 2102, 200001]
		enable_reward_shaping :	False
		env_debug_mode :	False
		agent1_learning_rate :	0.00025
		agent2_learning_rate :	0.00025
		a1_epsilon_start :	0.19
		a1_epsilon_end :	0.01
		a2_epsilon_start :	0.19
		a2_epsilon_end :	0.01
		batch_size :	32
		gamma :	0.99
		tau :	0.005
		sequence_length :	4
		max_replay_buffer_size :	100000
		cnn_a1 :	[(16, 3, 1, 1)]
		cnn_a2 :	[(16, 3, 1, 1)]
		fc_a1 :	[2688, 1024, 512, 256, 128]
		fc_a2 :	[2688, 1024, 512, 256, 128]



----------- Training Results ----------------
Started training at: 	2024-05-23  20:08:13
Ended training at: 	2024-05-23  20:08:32
Total training time:  0:00:19.539534
start_episode: 0
end_episode: 200
Episode count: 199
agent1 end epsilon: 0.01
agent2 end epsilon: 0.01
Draws: 0
agent_1_starts / agent_2_starts 1.173913043478261
agent_1_reward / agent_2_reward 1.247191011235955
Ave steps per game: 7.09
total_loss1 / num_steps1: 0.0
total_loss2 / num_steps2: 0.0
Input Parameters:
		start_episode :	0
		end_episode :	200
		console_status_interval :	500
		tensorboard_status_interval :	100
		ckpt_interval :	25000
		render_game_at :	[1, 2102, 200001]
		enable_reward_shaping :	False
		env_debug_mode :	False
		agent1_learning_rate :	0.00025
		agent2_learning_rate :	0.00025
		a1_epsilon_start :	0.19
		a1_epsilon_end :	0.01
		a2_epsilon_start :	0.19
		a2_epsilon_end :	0.01
		batch_size :	32
		gamma :	0.99
		tau :	0.005
		sequence_length :	4
		max_replay_buffer_size :	100000
		cnn_a1 :	[(16, 3, 1, 1)]
		cnn_a2 :	[(16, 3, 1, 1)]
		fc_a1 :	[2688, 1024, 512, 256, 128]
		fc_a2 :	[2688, 1024, 512, 256, 128]



----------- Training Results ----------------
Started training at: 	2024-05-23  20:09:35
Ended training at: 	2024-05-23  20:10:02
Total training time:  0:00:26.948307
start_episode: 0
end_episode: 200
Episode count: 199
agent1 end epsilon: 0.01
agent2 end epsilon: 0.01
Draws: 0
agent_1_starts / agent_2_starts 1.2222222222222223
agent_1_reward / agent_2_reward 0.8518518518518519
Ave steps per game: 19.22
total_loss1 / num_steps1: 0.0
total_loss2 / num_steps2: 0.0
Input Parameters:
		start_episode :	0
		end_episode :	200
		console_status_interval :	500
		tensorboard_status_interval :	100
		ckpt_interval :	25000
		render_game_at :	[1, 2102, 200001]
		enable_reward_shaping :	False
		env_debug_mode :	False
		agent1_learning_rate :	0.00025
		agent2_learning_rate :	0.00025
		a1_epsilon_start :	0.19
		a1_epsilon_end :	0.01
		a2_epsilon_start :	0.19
		a2_epsilon_end :	0.01
		batch_size :	32
		gamma :	0.99
		tau :	0.005
		sequence_length :	4
		max_replay_buffer_size :	100000
		cnn_a1 :	[(16, 3, 1, 1)]
		cnn_a2 :	[(16, 3, 1, 1)]
		fc_a1 :	[2688, 1024, 512, 256, 128]
		fc_a2 :	[2688, 1024, 512, 256, 128]



----------- Training Results ----------------
Started training at: 	2024-05-24  06:40:52
Ended training at: 	2024-05-24  06:41:05
Total training time:  0:00:13.113698
start_episode: 0
end_episode: 200
Episode count: 199
agent1 end epsilon: 0.01
agent2 end epsilon: 0.01
Draws: 0
agent_1_starts / agent_2_starts 0.8518518518518519
agent_1_reward / agent_2_reward 0.8181818181818182
Ave steps per game: 7.11
total_loss1 / num_steps1: 0.0
total_loss2 / num_steps2: 0.0
Input Parameters:
		start_episode :	0
		end_episode :	200
		console_status_interval :	500
		tensorboard_status_interval :	100
		ckpt_interval :	25000
		render_game_at :	[1, 2102, 200001]
		enable_reward_shaping :	False
		env_debug_mode :	False
		agent1_learning_rate :	0.00025
		agent2_learning_rate :	0.00025
		a1_epsilon_start :	0.19
		a1_epsilon_end :	0.01
		a2_epsilon_start :	0.19
		a2_epsilon_end :	0.01
		batch_size :	32
		gamma :	0.99
		tau :	0.005
		sequence_length :	4
		max_replay_buffer_size :	100000
		cnn_a1 :	[(16, 3, 1, 1)]
		cnn_a2 :	[(16, 3, 1, 1)]
		fc_a1 :	[2688, 1024, 512, 256, 128]
		fc_a2 :	[2688, 1024, 512, 256, 128]

