

# filename: h4_cnn.hyp
# training inputs for DDQN connect-four game player using NNs 
# which are constructed of CNN layers followed by FC layers
# NOTE: the equal sign is used as a delimeter, and should not exist 
# anywhere in this file except where hyperparameters are set

start_episode = 0
end_episode = 20000
console_status_interval = 500
tensorboard_status_interval = 100
ckpt_interval = 2500
render_game_at = [1, 250001]
enable_reward_shaping = True
env_debug_mode = False


agent1_learning_rate = 0.00025
agent2_learning_rate = 0.00025
a1_epsilon_start = .91
a1_epsilon_end = 0.01
a2_epsilon_start = .91
a2_epsilon_end = 0.01
batch_size = 32
gamma = 0.99
max_replay_buffer_size = 1000000


# Convolutional layers: (out_channels, kernel_size, stride, padding)
cnn_a1 = [(8, 3, 1, 1), (16, 3, 1, 1)]
cnn_a2 = [(8, 3, 1, 1), (16, 3, 1, 1)]

# Fully connected layer dimensions for each agent
fc_a1 = [672, 32, 8]
fc_a2 = [672, 32, 8]

NOTE: Calculate the output size for each convolutional layer using the formula:
CNN_Output_Size is equal to ((Input_Size−Kernel_Size+2×Padding)/Stride)+1
Make sure the FC_input_size matches the CNN_output_Size




----------- Training Results ----------------
Started training at: 	2024-05-13  07:31:28
Ended training at: 	2024-05-13  07:31:37
Total training time:  0:00:08.561085
start_episode: 0
end_episode: 20000
Episode count: 200
agent1 end epsilon: 0.8696670710633111
agent2 end epsilon: 0.8696670710633111
Draws: 0
agent_1_starts / agent_2_starts 0.8785046728971962
agent_1_reward / agent_2_reward 0.819337781694574
Ave steps per game: 20.44
total_loss1 / num_steps1: 0.08890233151614665
total_loss2 / num_steps2: 0.0890127420425415
Input Parameters:
		start_episode :	0
		end_episode :	20000
		console_status_interval :	500
		tensorboard_status_interval :	100
		ckpt_interval :	2500
		render_game_at :	[1, 250001]
		enable_reward_shaping :	True
		env_debug_mode :	False
		agent1_learning_rate :	0.00025
		agent2_learning_rate :	0.00025
		a1_epsilon_start :	0.91
		a1_epsilon_end :	0.01
		a2_epsilon_start :	0.91
		a2_epsilon_end :	0.01
		batch_size :	32
		gamma :	0.99
		max_replay_buffer_size :	1000000
		cnn_a1 :	[(8, 3, 1, 1), (16, 3, 1, 1), (32, 3, 1, 1)]
		cnn_a2 :	[(8, 3, 1, 1), (16, 3, 1, 1), (32, 3, 1, 1)]
		fc_a1 :	[672, 32, 8]
		fc_a2 :	[672, 32, 8]
models saved for both agents:
./wts/model1_h3_cnn.wts
./wts/model2_h3_cnn.wts
replay buffer saved to
./wts/replay_buffer_h3_cnn.pkl

**Exited training loop early at episode 200
start_episode = 200



----------- Training Results ----------------
Started training at: 	2024-05-13  07:35:02
Ended training at: 	2024-05-13  07:35:11
Total training time:  0:00:08.957901
start_episode: 200
end_episode: 20000
Episode count: 400
agent1 end epsilon: 0.8696670710633111
agent2 end epsilon: 0.8696670710633111
Draws: 0
agent_1_starts / agent_2_starts 0.9705882352941176
agent_1_reward / agent_2_reward 0.9115277961177869
Ave steps per game: 21.45
total_loss1 / num_steps1: 0.07519975621253253
total_loss2 / num_steps2: 0.09304362908005714
Input Parameters:
		start_episode :	200
		end_episode :	20000
		console_status_interval :	500
		tensorboard_status_interval :	100
		ckpt_interval :	2500
		render_game_at :	[1, 250001]
		enable_reward_shaping :	True
		env_debug_mode :	False
		agent1_learning_rate :	0.00025
		agent2_learning_rate :	0.00025
		a1_epsilon_start :	0.91
		a1_epsilon_end :	0.01
		a2_epsilon_start :	0.91
		a2_epsilon_end :	0.01
		batch_size :	32
		gamma :	0.99
		max_replay_buffer_size :	1000000
		cnn_a1 :	[(8, 3, 1, 1), (16, 3, 1, 1), (32, 3, 1, 1)]
		cnn_a2 :	[(8, 3, 1, 1), (16, 3, 1, 1), (32, 3, 1, 1)]
		fc_a1 :	[672, 32, 8]
		fc_a2 :	[672, 32, 8]
models saved for both agents:
./wts/model1_h3_cnn.wts
./wts/model2_h3_cnn.wts
replay buffer saved to
./wts/replay_buffer_h3_cnn.pkl

**Exited training loop early at episode 400
start_episode = 400



----------- Training Results ----------------
Started training at: 	2024-05-13  07:39:48
Ended training at: 	2024-05-13  07:55:26
Total training time:  0:15:37.457893
start_episode: 400
end_episode: 20000
Episode count: 19999
agent1 end epsilon: 0.010944119533693187
agent2 end epsilon: 0.010944119533693187
Draws: 8
agent_1_starts / agent_2_starts 0.9981649505556122
agent_1_reward / agent_2_reward 1.8802831555112751
Ave steps per game: 15.05
total_loss1 / num_steps1: 0.021101671503856778
total_loss2 / num_steps2: 0.013850497857977947
Input Parameters:
		start_episode :	400
		end_episode :	20000
		console_status_interval :	500
		tensorboard_status_interval :	100
		ckpt_interval :	2500
		render_game_at :	[1, 250001]
		enable_reward_shaping :	True
		env_debug_mode :	False
		agent1_learning_rate :	0.00025
		agent2_learning_rate :	0.00025
		a1_epsilon_start :	0.91
		a1_epsilon_end :	0.01
		a2_epsilon_start :	0.91
		a2_epsilon_end :	0.01
		batch_size :	32
		gamma :	0.99
		max_replay_buffer_size :	1000000
		cnn_a1 :	[(8, 3, 1, 1), (16, 3, 1, 1)]
		cnn_a2 :	[(8, 3, 1, 1), (16, 3, 1, 1)]
		fc_a1 :	[672, 32, 8]
		fc_a2 :	[672, 32, 8]
models saved for both agents:
./wts/model1_h4_cnn.wts
./wts/model2_h4_cnn.wts
replay buffer saved to
./wts/replay_buffer_h4_cnn.pkl

**Exited training loop early at episode 19999
start_episode = 19999

