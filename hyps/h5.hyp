

# filename: h5.hyp
# training inputs for DDQN connect-four game player using NNs 
# which are constructed of CNN layers followed by FC layers
# NOTE: the equal sign is used as a delimeter, and should not exist 
# anywhere in this file except where hyperparameters are set

start_episode = 0
end_episode = 200002
console_status_interval = 500
tensorboard_status_interval = 100
ckpt_interval = 25000
render_game_at = [1, 200001]
enable_reward_shaping = True
env_debug_mode = False


agent1_learning_rate = 0.00025
agent2_learning_rate = 0.00025
a1_epsilon_start = .99
a1_epsilon_end = 0.01
a2_epsilon_start = .99
a2_epsilon_end = 0.01
batch_size = 32
gamma = 0.99
tau = 0.005
max_replay_buffer_size = 100000


# Convolutional layers: (out_channels, kernel_size, stride, padding)
cnn_a1 = [(16, 3, 1, 1)]
cnn_a2 = [(16, 3, 1, 1)]

cnn_to_lstm_fc_size = 672

# LSTM layer parameters: (lstm_layers, lstm_hidden_size)
lstm_a1 = [(1, 672)] 
lstm_a2 = [(1, 672)]

# Fully connected layer dimensions for each agent
fc_a1 = [672, 256, 128, 64, 32]  
fc_a2 = [672, 256, 128, 64, 32]

 
sequence_length = 3


NOTE: Calculate the output size for each convolutional layer using the formula:
CNN_Output_Size is equal to ((Input_Size−Kernel_Size+2×Padding)/Stride)+1
Make sure the FC_input_size matches the CNN_output_Size



----------- Training Results ----------------
Started training at: 	2024-05-17  12:54:34
Ended training at: 	2024-05-17  13:09:59
Total training time:  0:15:24.315774
start_episode: 0
end_episode: 100002
Episode count: 100001
agent1 end epsilon: 0.01
agent2 end epsilon: 0.01
Draws: 14
agent_1_starts / agent_2_starts 1.000480105623237
agent_1_reward / agent_2_reward 1.0199228338306598
Ave steps per game: 7.04
total_loss1 / num_steps1: 0.0
total_loss2 / num_steps2: 0.0
Input Parameters:
		start_episode :	0
		end_episode :	100002
		console_status_interval :	500
		tensorboard_status_interval :	100
		ckpt_interval :	25000
		render_game_at :	[1, 100001]
		enable_reward_shaping :	True
		env_debug_mode :	False
		agent1_learning_rate :	0.00025
		agent2_learning_rate :	0.00025
		a1_epsilon_start :	0.99
		a1_epsilon_end :	0.01
		a2_epsilon_start :	0.99
		a2_epsilon_end :	0.01
		batch_size :	32
		gamma :	0.99
		tau :	0.005
		max_replay_buffer_size :	100000
		cnn_a1 :	[(16, 2, 1, 1)]
		cnn_a2 :	[(16, 2, 1, 1)]
		cnn_to_lstm_fc_size :	896
		lstm_a1 :	[(1, 896)]
		lstm_a2 :	[(1, 896)]
		fc_a1 :	[896, 256, 128, 64, 32]
		fc_a2 :	[896, 256, 128, 64, 32]
		sequence_length :	3



----------- Training Results ----------------
Started training at: 	2024-05-17  14:18:31
Ended training at: 	2024-05-17  15:03:41
Total training time:  0:45:09.654060
start_episode: 0
end_episode: 200002
Episode count: 200001
agent1 end epsilon: 0.01
agent2 end epsilon: 0.01
Draws: 45
agent_1_starts / agent_2_starts 1.0018617127757536
agent_1_reward / agent_2_reward 1.0112249054057674
Ave steps per game: 7.11
total_loss1 / num_steps1: 0.0
total_loss2 / num_steps2: 0.0
Input Parameters:
		start_episode :	0
		end_episode :	200002
		console_status_interval :	500
		tensorboard_status_interval :	100
		ckpt_interval :	25000
		render_game_at :	[1, 200001]
		enable_reward_shaping :	True
		env_debug_mode :	False
		agent1_learning_rate :	0.00025
		agent2_learning_rate :	0.00025
		a1_epsilon_start :	0.99
		a1_epsilon_end :	0.01
		a2_epsilon_start :	0.99
		a2_epsilon_end :	0.01
		batch_size :	32
		gamma :	0.99
		tau :	0.005
		max_replay_buffer_size :	100000
		cnn_a1 :	[(16, 3, 1, 1)]
		cnn_a2 :	[(16, 3, 1, 1)]
		cnn_to_lstm_fc_size :	672
		lstm_a1 :	[(1, 672)]
		lstm_a2 :	[(1, 672)]
		fc_a1 :	[672, 256, 128, 64, 32]
		fc_a2 :	[672, 256, 128, 64, 32]
		sequence_length :	3

