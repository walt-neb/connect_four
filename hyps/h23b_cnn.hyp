


#filename: h23a_cnn.hyp

# used for RL training of DDQN for connect-four game play with CNN layers

start_episode = 0
end_episode = 100002
max_replay_buffer_size = 1000000

agent1_learning_rate = 0.00025
agent2_learning_rate = 0.00025

# Convolutional layer configurations: (out_channels, kernel_size, stride, pading)
cnn_a1 = [(16, 3, 1, 1), (32, 3, 1, 1)]
cnn_a2 = [(16, 3, 1,1 ), (32, 3, 1, 1)]

# Fully connected layer dimensions for each agent
fc_a1 = [1344, 16]
fc_a2 = [1344, 16]

NOTE: Calculate the output size for each convolutional layer using the formula:
CNN_Output_Size is equal to ((Input_Size−Kernel_Size+2×Padding)/Stride)+1
Make sure the FC_input_size matches the CNN_output_Size



console_status_interval = 500
tensorboard_status_interval = 100
ckpt_interval = 2500

render_game_at = [1, 250001]

a1_epsilon_start = 0.5
a1_epsilon_end = 0.01
a2_epsilon_start = 0.5
a2_epsilon_end = 0.01

batch_size = 32
gamma = 0.99



----------- Training Results ----------------
Started training at: 	2024-05-11  15:55:17
Ended training at: 	2024-05-11  16:08:44
Total training time:  0:13:26.535793
start_episode: 0
end_episode: 100002
Episode count: 5500
A1 Convolutional layers: [(16, 3, 1, 1), (32, 3, 1, 1)]
A2 Convolutional layers: [(16, 3, 1), (32, 3, 1)]
A1 Fully connected layers: [1344, 16]
A2 Fully connected layers: [192, 16]
agent1 end epsilon: 0.16961402762974603
agent2 end epsilon: 0.16961402762974603
Draws: 0
Comp/Ratio Agent_1_to_Agent_2 Reward 1.2695431407965094
Ave steps per game: 22.77
total_loss1 / num_steps1: 0.05515559194119353
total_loss2 / num_steps2: 0.19952844580014548
agent1 lr: 0.00025
agent2 lr: 0.00025
gamma: 0.99
batch_size: 32
buffer_capacity: 1000000

